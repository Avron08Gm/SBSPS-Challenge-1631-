{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "^C\nRequirement already satisfied: pytorch-lightning==0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.1)\nRequirement already satisfied: timm in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.30)\nRequirement already satisfied: wandb in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.1)\nRequirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (1.18.2)\nRequirement already satisfied: future>=0.17.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (0.18.2)\nRequirement already satisfied: PyYAML>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (5.3.1)\nRequirement already satisfied: torch>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (1.4.0)\nRequirement already satisfied: tqdm>=4.41.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (4.46.0)\nRequirement already satisfied: tensorboard>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-lightning==0.8.1) (2.2.2)\nRequirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (0.5.0)\nRequirement already satisfied: configparser>=3.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (5.0.0)\nRequirement already satisfied: nvidia-ml-py3>=7.352.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (7.352.0)\nRequirement already satisfied: six>=1.10.0 in c:\\users\\s5678\\appdata\\roaming\\python\\python37\\site-packages (from wandb) (1.12.0)\nRequirement already satisfied: subprocess32>=3.5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (3.5.4)\nRequirement already satisfied: requests>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (2.23.0)\nRequirement already satisfied: GitPython>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (3.1.3)\nRequirement already satisfied: sentry-sdk>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (0.15.1)\nRequirement already satisfied: Click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (7.1.2)\nRequirement already satisfied: gql==0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (0.2.0)\nRequirement already satisfied: watchdog>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (0.10.2)\nRequirement already satisfied: psutil>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (5.7.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\nRequirement already satisfied: shortuuid>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (1.0.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (2.8.1)\nRequirement already satisfied: absl-py>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (0.9.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (0.34.2)\nRequirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (46.1.3.post20200330)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (1.6.0.post3)\nRequirement already satisfied: grpcio>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (1.28.1)\nRequirement already satisfied: protobuf>=3.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (3.11.3)\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (1.13.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (0.4.1)\nRequirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch-lightning==0.8.1) (3.2.1)\nRequirement already satisfied: pillow>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (7.1.2)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (2020.4.5.1)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (1.25.8)\nRequirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (3.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\nRequirement already satisfied: promise<3,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql==0.2.0->wandb) (2.3)\nRequirement already satisfied: graphql-core<2,>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gql==0.2.0->wandb) (1.1)\nRequirement already satisfied: pathtools>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1) (0.2.8)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1) (4.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1) (4.1.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.1) (1.3.0)\nRequirement already satisfied: smmap<4,>=3.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.8.1) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.8.1) (3.1.0)\n"
    }
   ],
   "source": [
    "!pip install pytorch-lightning==0.8.1 timm wandb\n",
    "!git clone https://github.com/davidtvs/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install\n",
    "!wget https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
    "!wget https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
    "!wget https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt\n",
    "!wget https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl\n",
    "!pip install torch-lr-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import classification_report\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base') # Tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"distilroberta-base\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"All is well\" # Checking the tokenization\n",
    "enc = tokenizer.encode_plus(t)\n",
    "token_representations = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\n",
    "print(enc[\"input_ids\"])\n",
    "print(tokenizer.decode(enc[\"input_ids\"]))\n",
    "print(f\"Length: {len(enc['input_ids'])}\")\n",
    "print(token_representations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeninzing class\n",
    "class TokenizersCollateFn:\n",
    "    def __init__(self, max_tokens=512):\n",
    "\n",
    "        t = ByteLevelBPETokenizer(\n",
    "            \"tokenizer/vocab.json\",\n",
    "            \"tokenizer/merges.txt\"\n",
    "        )\n",
    "        t._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        t.enable_truncation(max_tokens)\n",
    "        t.enable_padding(max_length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n",
    "        self.tokenizer = t\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
    "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
    "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
    "        labels = torch.tensor([x[1] for x in batch])\n",
    "        \n",
    "        return (sequences_padded, attention_masks_padded), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n",
    "val_path = \"val.txt\"\n",
    "\n",
    "label2int = {\n",
    "  \"sadness\": 0,\n",
    "  \"joy\": 1,\n",
    "  \"love\": 2,\n",
    "  \"anger\": 3,\n",
    "  \"fear\": 4,\n",
    "  \"surprise\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_from_pickle(directory):\n",
    "    return pickle.load(open(directory,\"rb\"))\n",
    "data = load_from_pickle(directory=\"merged_training.pkl\")\n",
    "\n",
    "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "data= data[data[\"emotions\"].isin(emotions)]\n",
    "data = data.sample(n=20000);\n",
    "\n",
    "data.emotions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.emotions.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "input_train, input_val, target_train, target_val = train_test_split(data.text.to_numpy(), \n",
    "                                                                    data.emotions.to_numpy(), \n",
    "                                                                    test_size=0.2)\n",
    "\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val, target_val, test_size=0.5)\n",
    "\n",
    "\n",
    "train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\n",
    "val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\n",
    "test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\n",
    "final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
    "\n",
    "train_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\n",
    "val_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\n",
    "test_dataset.to_csv(val_path, sep=\";\",header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.data_column = \"text\"\n",
    "        self.class_column = \"class\"\n",
    "        self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                               engine=\"python\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "        \n",
    "ds = EmoDataset(train_path)\n",
    "ds[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "  \n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size), # For a linear custom model layer\n",
    "            Mish(),                                                      # For some kind of activation fucntion.\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "        \n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02) # For initializaing the linear layer weights \n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_() # Making the biaz zero\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask) # Feeding the token input to the base_model to transform\n",
    "        \n",
    "\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "        self.loss = nn.CrossEntropyLoss() # combine LogSoftmax() and NLLLoss()\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(X), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        return self.model(X, *args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmoDataset(ds_path),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    collate_fn=TokenizersCollateFn()\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finding the best LR\n",
    "lr=0.1 ## uper bound LR\n",
    "from torch_lr_finder import LRFinder\n",
    "hparams_tmp = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    epochs=1,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1,\n",
    ")\n",
    "module = TrainingModule(hparams_tmp)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(module.parameters(), lr=5e-7) ## lower bound LR\n",
    "lr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100, accumulation_steps=hparams_tmp.accumulate_grad_batches)\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder.plot(show_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presetting the LR\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    epochs=2,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus = 1,max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        (X, attn), y = batch_\n",
    "        batch = (X.cuda(), attn.cuda())\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        y_pred = torch.argmax(module(batch), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=len(emotions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving \n",
    "model = module.model\n",
    "torch.save(model.state_dict(), 'roberta_trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for loading the model into the notebook\n",
    "# This can only be involked when we have exeuted and compiled the model structure beforehand\n",
    "model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "model.load_state_dict(torch.load('roberta_trained')) # Loading the saved state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'This is deeply regretted'      # custom string can be placed here for prediction\n",
    "enc = tokenizer.encode_plus(t)\n",
    "X = torch.tensor(enc[\"input_ids\"]).unsqueeze(0)\n",
    "attn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0)\n",
    "torch.argmax( model( (X, attn ) ))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "intel_dc2edge",
   "display_name": "Python (intel_dc2edge)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}